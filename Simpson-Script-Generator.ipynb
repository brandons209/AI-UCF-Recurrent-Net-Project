{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new Simpson scripts with LSTM RNN\n",
    "In this project, we will be using an LSTM with the help of an Embedding layer to train our network on an episode from the Simpsons, specifically the episode \"Moe's Tavern\". This is taken from [this](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data) dataset on kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets import all of our libraries we need. We utilize Keras' Tokenizer method for tokenizing our inputs, and pad_sequences for generating our sequences. Our embedding layer has a fixed input size, so instead of passing our entire script at once we supple a sequence of characters with a length we can choose. Documentation can be found for [tokenizer](https://keras.io/preprocessing/text/) and [pad_sequences](https://keras.io/preprocessing/sequence/).\n",
    "\n",
    "I have also created a helper function to handle some loading and saving of dictionaries we will make, and tokenizing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#general imports\n",
    "import helper #\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#pre-processing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense\n",
    "from keras.layers import Embedding, LSTM\n",
    "\n",
    "#training\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import optimizers as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset statistics\n",
    "Before starting our project, we should take a look at the data we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Dataset Stats-----------\n",
      "Approximate number of unique words: 11492\n",
      "Number of scenes: 262\n",
      "Average number of sentences in each scene: 15\n",
      "Number of lines: 4258\n",
      "Average number of words in each line: 12\n"
     ]
    }
   ],
   "source": [
    "script_text = helper.load_script('data/moes_tavern_lines.txt')\n",
    "\n",
    "print('----------Dataset Stats-----------')\n",
    "print('Approximate number of unique words: {}'.format(len({word: None for word in script_text.split()})))\n",
    "scenes = script_text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {:.0f}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {:.0f}'.format(np.average(word_count_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
